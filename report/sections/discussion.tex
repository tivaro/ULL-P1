\section{Discussion}

Of all the experiments, the initialisation strategy shows the largest effect on performance. Although initialising the corpus with the true segmentation results in the highest probability before sampling, a higher probability is reached using other initialisations. The fact that there are higher probabilities than the true segmentation does indicate a fundamental limitation of our model.

A worrying finding is the fact that initialisation has a large effect on the corpus probability after running the sampler. This indicates that our sampler does not converge to a global optimum but get's stuck locally. Furthermore, this finding is not in line with \cite{Goldwater200921} who found no effect of initialisation. 

Moreover, we note that although the initialization with true boundaries has the highest joint probability on the corpus initially, it does not get the highest final joint probability. Assuming the true segmentation of the corpus is correct, we conclude that the model is in fact inadequate for this problem. This suggests that the assumption that words are independent units may not be a realistic assumption, and should be dropped or changed (to e.g. the bigram assumption). 

Interestingly, the probability of the true initialisation does not change after sampling, but remains constant indicating that the true segmentation is in fact a local optimum. Furthermore, fewer boundaries result in a higher probability with the best probability found when no boundaries are initialised. This initialisation corresponds to the trival MLE segmentation where each utterance is seen as a token.

Varying the parameters only has marginal effects. The temperature regime can slightly improve learning speed and performance, but it does not make a big difference. The $\alpha_0$ parameter should not be too small, but after a certain value (e.g. $\alpha_0 > 20$), it does not really affect retrieval quality anymore. The same applies to the $p_\#$ parameter, where values greater than 0.5 are preferable. Finally, the fact that the lines in most graphs don't seem to converge, suggests that the model may be stuck in a local optimum.

The numeric results of our experiments do not directly match those of \cite{Goldwater200921}, but they are similar. We suspect that these differences may be caused by small implementation differences. However, our model suffers from the same problem as their unigram model: it tends to under-segment the corpus. Thus, our results confirm an important finding of \cite{Goldwater200921}, which is that the assumption that words are independent of each other is most likely inadequate.

Taken altogether, the DP unigram model does not perform very well on this task, in line with the findings of  \cite{Goldwater200921}. We suggested the PYP as an improvement to this model, but in fact, the PYP model worsened performance. Either the model and parameters were not implemented correctly, or the PYP model is actually not suited for this task. We conducted several experiments to investigate both options.

With $\beta$ set to 0, the model behaves as the DP model, as expected. This indicates that the of modelling $h^-$ is implemented correctly. Another expected finding is that higher $\beta$ values result in an increased number of clusters ($K$). 

However we also found some unexpected effects of the $\beta$ parameter. First of al, lower the values of $\beta$ results in decreased performance. As the highest performance was found for $\beta = 0$, this finding is counter intuitive, and may indicate an implementation error.

As expected, larger $\beta$ values result in more clusters. The number of types also increases with $\beta$, which means that not only do we have more clusters per type, but there also are more different labels. Surprisingly the number of tokens increases with $\beta$ as well. Taken together with the qualitative results, we conclude that the model does still under-segment but also makes many incorrect segmentations.

The different values of $\beta$ also have a large effect on corpus probability, even after initialisation. This may mean indicate an implementation error and may be due to the fact that we defined the corpus probability as $p(\vt{w}|\vt{z})$ instead of including the distribution over $\vt{z}$. However the same probability is used in \cite{Goldwater200921} for the bigram model. Analogous to our PYP model, the seating is modelled explicitly for the bigram model. The probability of \cite{Goldwater200921} does converge nicely, which means that our equations for $p(w_i | \wmin, \vt{z}^-)$ may be incorrect.
 
 Besides possible implementation errors, our finding may imply that the PYP model is not suited for this task. It could be the case that the distribution of types and tokens in infant directed language is not distributed according to the power-law. In fact, this may very well be the case, as the vocabulary of our corpus is rather limited. 
 Furthermore, our findings may indicate that the assumption of the distribution over words that is implied by the PYP is not an assumption that an ideal learner should make in order to find the right clustering. However, more sanity checks and control experiments have to be conducted to find out what exactly is causing our results.