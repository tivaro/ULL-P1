\section{Model}

\subsection{DP model}

The probabilities of the seating according to the PYP are: \eqref{eq:DP}
\subsection{PYP Model}
\subsubsection{Model definitions}

The probabilities of the seating according to the PYP are: \eqref{eq:PYP}

Now we include the label and the base distribution $P_0$ in the model:

\begin{align}
p(w_i=\ell|\zmin,\vt{\ell}(\zmin),\alpha) &= \sum_{k=1}^{K(\zmin)}\mathcal{I}(\ell_k=\ell)\dfrac{n_k^{(\zmin)}-\beta}{i - 1 + \alpha} + P_0(\ell)\dfrac{\alpha + \beta K}{i -1 -\alpha} \\
&= \dfrac{n_{\ell}^{(\wmin)}- n_{\ell}^{\vt{\ell}(\zmin)}\beta}{i - 1 + \alpha} + P_0(\ell)\dfrac{\alpha + \beta K}{i -1 -\alpha} \\
&= \dfrac{P_0(\ell)\alpha + n_{\ell}^{(\wmin)}+ \beta (K - n_{\ell}^{\vt{\ell}(\zmin)})}{i - 1 + \alpha} 
\end{align}

Where $n_{\ell}^{(\wmin)}$ is the number of times the label $\ell$ occurs in the segmented corpus and $n_{\ell}^{\vt{\ell}(\zmin)}$ is the number of tables where label $\ell$ occurs.

\subsubsection{Inference}

For gibs sampling, we pick a random boundary and propose two hypothesis:
\\
$h_1$: There is no boundary at this location \\
$h_2$: There is a boundary at this location


$h^-$ denotes the set of words and seating arrangement shared by the both hypothesis.

In order to find $h^-$, we remove the word (or words) from 

\begin{align}
P(h_1 | h^-) &= P(w_1|h^-)P(u_{w_1} | h^{-}) \\
&= \dfrac{P_0(w_1)\alpha + n_{w_1}^{(\wmin)}+ \beta (K - n_{w_1}^{\vt{\ell}(\zmin)})}{n^- + \alpha}
\dfrac{n_u^{(h^-)} + \frac{\rho}{2}}{n^- + \rho}
\end{align}

\begin{align}
P(h_2 | h^-) &= P(w_2,w_3|h^-)\\
=&P(w_2|h^-)P(u_{w_2}|h^1)P(w_3 | w_2,h^-)P(u_{w_3}|u_{w_2}|h^-) \\
=& \dfrac{P_0(w_2)\alpha + n_{w_2}^{(\wmin)}+ \beta (K - n_{w_2}^{\vt{\ell}(\zmin)})}{n^- + \alpha}
\dfrac{n^- - n_\$^{(h^-)} + \frac{\rho}{2}}{n^- + \rho} \\
&\cdot 
\dfrac{P_0(w_3)\alpha + n_{w_3}^{(\wmin)}+ \mathcal{I}(w_2 = w_3) \beta (K - n_{w_3}^{\vt{\ell}(\zmin)})}{n^- + \alpha + 1} \\
&\cdot
\dfrac{n_u^{(h^-)} + \mathcal{I}(w_2 = w_3)  + \frac{\rho}{2}}{n^- + 1 + \rho}
\end{align}



\begin{algorithm}[H]
	\caption{Pseudo algorithm}
    \label{alg:quad}
    \nonl  \SetSideCommentLeft \emph{Initialisation} \\
    $\textit{segmentation} \gets \text{initialise randomly}$ \\ 
    $seating \gets \emptyset$ \\
    
    \For{all words $w_i$ in segmentation}{
	addCustomer($w_i$)
     } 
     
     \nonl \SetSideCommentLeft \emph{Gibbs Sampling}

   \For{all possible boundaries $b_i$}{
	$w_1 \gets$ word if boundary is not placed (h1) \\
	$w_2, w_3 \gets$ word if boundary is  placed (h2) \\
	\lIf {boundary $b_i$ currently exists}{
		removeCustomer($w_2$) \\
		removeCustomer($w_3$)
		}
	\lElse{
		removeCustomer($w_1$)	
	}

	calculate $p(h1)$ \\
	calculate $p(h2)$
	
	$insert\_boundary_i \gets$ sample proportionally
	\\
	\lIf{$insert\_boundary_i$}{
		addCustomer($w_2$)
		addCustomer($w _3$)
		}
	\lElse{
		addCustomer($w_1$)	
	}
}
  \end{algorithm}
  
  	
where addCustomer adds customers to a table proportional to (2) \\
and removeCustomer selects one of the tables of the word, proportional to it's count. \\
\\
Both functions will also update K, and remove the tables if they have become empty