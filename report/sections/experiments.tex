\section{Experiments}

\subsection{Evaluation metrics}

Following \cite{Goldwater200921}, we define two types of evaluation metrics: the joint probability of the corpus and retrieval measures.

\subsubsection{Corpus probability}

For the DP model, the joint probability over all the words in the corpus is defined for as follows:

\begin{equation}
p(\mathbf{w} | \alpha, P_0) = \prod_{w_i \in \mathbf{V}} \left( \frac{n_{w_i} - 1 + \alpha P_0(w_i)}{N - 1 + \alpha} \right)^{n_{w_i}}
\end{equation}

where $\mathbf{V}$ is the lexicon or vocabulary, $N$ is the total number of words in the corpus, and $n_{w_i}$ is the number of occurrences of word $w_i$ in the corpus.

\subsubsection{Retrieval measures}

We assess the quality of the retrieved segmentation using precision, recall and the $F_0$ measure. Like \cite{Goldwater200921}, we evaluate these measures on words (per utterance), on boundaries (per utterance, excluding the start and end of the utterance), and on the lexicon. This gives us nine retrieval measures for each experiment.

\subsection{$P_0$ distribution}

$P_0$ is the prior distribution over phonemes. We experiment with a uniform distribution ('uniform'), and one that is based on proportional counts in the corpus ('mle'). We evaluate using the log joint probability over time.

\subsection{Gibbs sampling}

The retrieved segmentation depends in part on the Gibbs sampling procedure. We experiment with different temperature regimes and initialisation strategies. Like the choice of $P_0$ distribution, we evaluate the temperature regimes and initialisation strategies using the log joint probability over time.

\subsubsection{Temperature regime}

We experimented with three different temperature regimes:

\begin{itemize}
\item Regime 0: 20000 iterations, from 0.1 to 1 in evenly spaced steps of 0.1
\item Regime 1: 30000 iterations, from 0.1 to 1.5 in evenly spaced steps of 0.1
\item Regime 2: 40000 iterations, from 0.002 to 1 in evenly spaced steps of 0.002
\end{itemize}

\subsubsection{Initialisation}

We experiment with initialisation with the true word boundaries as defined by the corpus as well as random boundary initialisation. In the random initialisation experiments, we first remove all boundaries and then randomly generated new boundaries. We experimented with different proportions of boundaries that were initialised with respect to the total number of possible boundaries (per utterance). We tested proportions $0$, $\frac{1}{3}$, $\frac{2}{3}$, $1$. The default was set to random initialisation with a fraction of $\frac{2}{3}$, as this was close to the true proportion in the corpus (29%).

\subsection{Model parameters}

The parameters of the DP model are $\alpha_0$, which affects the number of word types proposed, and $p_\#$, the prior probability of a word boundary. We experiment with $\alpha_0 \in \{1, 2, 5, 10, 20, 50, 100, 200, 500\}$ and $p_\# \in \{0.1, 0.3, 0.5, 0.7, 0.9\}$. During the $\alpha_0$ experiments we kept $p_\#$ fixed to 0.5 and during the $p_\#$ experiments we kept $\alpha_0$ fixed to 20.

\subsection{PYP Model}

Because $h^{-}$ has to be modelled explicitly in the PYP model, the sampling is computationally a lot more expensive than the sampler used in DP sampling. Therefore a different iteration scheme was used with only 4000 iterations, and three equally long temperature steps (0.1, 1.1, 1.6).

\subsubsection{Algorithm}
Because of the different implementation of the Gibbs sampler in the PYP model, we first ran some control experiments, to confirm our implementation.
The PYP model was run with the $\beta$ parameter set to 0 and the results were compared to the DP model with the exact same parameters and temperature regime. Both models were expected to produce the same results.

Next, we examined the corpus probability across iterations, to confirm that our Gibbs sampler was reaching an optimum.

\subsubsection{Parameters}
Both the concentration parameter $\alpha$ and the smoothing parameter $\beta$ affect the seating distribution. Because of this possible interaction, there might be no single best values for $\alpha$ and $\beta$. In order to find the best combination for both paramters, a grid search was conducted with $\alpha \in \{1, 2, 5, 10, 20, 50, 100, 500 \}$ and $\beta \in \{ 0.01, 0.1, 0.2, 0.4,0.6, 0.8, 1, 0 \}$
